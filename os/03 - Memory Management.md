# 1. Hierarchy & Organization of Memory
**Virtual memory** consists of an addition to the memory hierarchy where a translation of memory addresses (virtual and physical) occurs using a processing unit (MMU). It and the disk are managed by the operating systems.

When you want to access a position that has not been brought from the disk (page fault), a “swap” (expensive) is performed to bring that position to a partition (Linux) or file (Windows).

![[Pasted image 20240521093301.png]]

# 2. Address Translation
## 2.1 Addressing space
- **Logical space**: addresses the processor provides kept in the general register of addresses. Size of logical space valid for a process.
- **Physical space**: the memory the computer can interpret. Size of valid physical space of the system.
- **Memory Management Unit (MMU)**: Translates logical into physical addresses
![[Pasted image 20240521100827.png]]

## 2.2 Address translation
The code of a program goes through several stages before its execution in main memory:
- **Compilation (Symbolic Names)**: Separates text from data; converts the instructions generated by the code into assembly code (.o).
- **Linking (Virtual address space)**: Creates sections of text and data (adds the stack); gathers codes from object files (assembler) for the interaction of the files (libraries and functions) and recalculates the relative positions.
- **Loading (Relocation of the virtual address space)**: Initial addresses are assigned to sections of the program and initialize variables, table, stack... An image of the process is generated.
- **Execution (Physical address space)**: where virtual memories are translated into physical ones.

This facilitates abstraction and portability to other devices.

![[Pasted image 20240521101902.png]]
# 3. Memory Models
## 3.1 According to the processes in main memory
### 3.1.1 Uniprogram (primitive)
**Only one process** can be in main memory, it could be non-relocatable, so the program will not have the physical absolute addresses equal to the virtual ones; or relocatable, with physical addresses that are not the same as those of the programs.

The MMU did not process translations, but checks memory protection depending on whether a position in the range is being accessed or not. In kernel mode there would be no exceptions.

### 3.1.2 Multiprogram
Several processes can be located in main memory, causing memory protection and allocation problems. The latter could be solved by **partitioning** the memory statically or **dynamically**, with a bitmap or linked lists.

The memory allocation policy can be:
- *Fixed partition*: with queues to allocate per partition or a single queue that allocates wherever it fits.
- *Variable partitioning*: 
	- Best-Fit, the partition where you would leave the least space.
	- Worst-Fit, the partition where I would leave the most space.
	- First-Fit, the first memory slot that fits.

Static partitions entail problems, either internal ones, when there is space left inside one, or external ones, leaving empty partitions (a problem that can be solved by **compacting** the external fragments if the process is relocatable).

Meanwhile, the MMU checks the range \[base, base+limit\] to solve the protection problem.

## 3.2 Depending on whether the process remains in the main memory in execution
### 3.2.1 Resident
The entire process must be in main memory from its intake to its output.
### 3.2.2 Non-resident
Allows the change of processes between memory and disk: **swap-out** ($MEM$ -> $Disk$) or **swap-in** ($Disk$ -> $MEM$). In this exchange this program is suspended. This allows you to enlarge the memory image by removing it from memory. This process is slow and worse if it is not relocatable.

Processes waiting for DMA cannot be removed from memory.

## 3.3 Depending on whether memory addresses can change
### 3.3.1 Relocatable
A process can be physically relocated during its execution. It allows the image of the process to be enhanced, as well as compaction. There are two ranges to consider, those of virtual addresses and physical addresses.
### 3.3.2 Non-relocatable
A process cannot be physically relocated during its execution.

## 3.4 According to memory space allocation
### 3.4.1 Contiguous
The memory image is located contiguously in main memory.
### 3.4.2 Non-contiguous
The memory image is located non-contiguously in main memory. There are two ways to implement the model (not mutually exclusive):
- *Segmentation*: Divide the space with different sizes of virtual addresses (data, texts, stack...).
- *Paging*: Divide the space with equal sizes of virtual addresses (4KB).

## 3.5 Depending on whether the entire image should always be in the main memory
### 3.5.1 Integer
The entire process is in main memory.
### 3.5.2 Non-integer
There is no need to save the entire process in main memory.

Based on the principle of locality, it is very likely when accessing a memory location, whether instructions or data, that the next query of a location will be close to the previous one; This principle motivates bringing nearby blocks into the cache.

The advantages are that processes with sizes larger than RAM are allowed and that swaps can be made more efficient.

Overlays, Dynamically Link Libraries (**DLL**) and virtual memory.

Using DDL, less memory is used and with less redundancy. However, by not having the code integrated, the executable loses portability.

# 4. Paging & Segmentation
In modern OSs the non-contiguous model is implemented. However, to solve the efficiency problems of accessing secondary memory, several solutions have been found.

## 4.1 Paging
It consists of dividing virtual addresses into pages of the same size, typically 4KB. External fragmentation is avoided, but internal fragmentation persists.

The correspondence of logical memory with physical memory is resolved with **an array for each process** managed by the operating system where the ith position of the page table contains the value of the frame number in physical memory.

The role of the MMU is to translate the logical address frame into the frame; while the offset (displacement) within a page is the same as in a frame. There are actually two memory accesses, that of the table and that of the desired address.

**Notation:**
![[Pasted image 20240709003451.png]]


![[Pasted image 20240709003226.png]]

When there is a context switch, it is saved as a pointer in the PTBR register where the array is.

There is an extra field, **V**, for memory protection with a value of 1 or 0 if it corresponds to the process.

### 4.1.1 Shared Paging
To avoid conflicts with other processes, add to each entry:
- Read-only, for text shared in editors, compilers, graphical interfaces...
- Copy-on-Write, in the case of fork, the pages are duplicated only if they are modified by either the parent process or the child process, otherwise they will be shared. A COW field is needed.
- Read-write, for communication between processes.
- Private, text and private data that should not be accessed by other processes.

### 4.1.2 Page Size

$$\text{\# pages} = \text{\# entries} = \frac{\text{size of logical space}}{\text{page size}}$$
$$\text{\# frames} = \frac{\text{physical address space}}{\text{page size}}$$
$$\text{\#frame}_{\text{bits}} = \log_2\ (\text{frames}) = \text{\# bit with the most significant 1}$$

$$\text{\# offset}_\text{bits} = \log_2 (\text{page size in bytes})$$
$$\text{table size}= \text{\#pages}\ \cdot\ (\text{frame}_\text{bits}\ +\ V) \text{ size of each entry}$$

#### 4.1.2.1 Page recursion
The array is also paged, so if the size of the array is larger than one frame, it will be divided into pages as well.

## 4.2 Pagination problems
The size may be too large, which can be solved with an inverted page table or hierarchical paging; while for memory accesses they can be resolved with a **Translation Lookaside Buffer** (TLB) cache.

### 4.2.1 Inverted page table
The inverted page table consists of creating a table for the physical address space; Although it is slow and difficult to implement, it is necessary to receive the pid and a data structure for the information of non-resident pages.

### 4.2.2 Hierarchical pagination / Multilevel pagination
Take advantage of the fact that many page table entries have the valid bit set to 0 to divide the page number into two fields:
- **2nd level**: equal to the algorithm of the number of entries. It will express such quantity in binary.
- **1st level**: the rest of the field of bits for the virtual address.

A table of pointers to page tables is created with much less memory occupation, although this leads to increased memory accesses.

## 4.3 Translation Lookaside Buffer (TLB)
It works as a page table cache so that after a first fault, it is saved in the TLB and significantly speeds up memory access. It is located in the MMU, where the TLB will be queried first and then the page table.

After a context switch, the TLB flushes its entries unless it has an associated PID.

## 4.4 Segmentation
The processor is fragmented into segments of variable size, passing through a segmentation table.

It can be combined with pagination to avoid external fragmentation.

# 5. Virtual Memory
As an extension of the memory hierarchy we find the possibility in non-integer models based on non-contiguous ones to make use of.

The transfer unit is the page (4KB).

Presence (P), validity (V) bits are used, as well as reference, permissions, Dirty...

A page fault consists of the exception of requesting a memory address that is not in main memory (P=0).

In Linux it is treated as a partition, while in Windows it is saved to a file (slower).

The policies for loading pages can be:
- On demand, only when needed
- Prepagination, when several pages are brought.
- Prefetch, in a page fault.

## 5.1 HW & SW Requirements
![[Pasted image 20240709011548.png]]

## 5.2 Allocation and replacement algorithms
To minimize page faults, several algorithms are applied to:
- **Optimal**: difficult to know the future, but it is by definition the best algorithm.
- **Random**: random.
- **FIFO**: First In, First Out.
	- Belady's anomaly, adding more frames in the FIFO policy can, in some cases, cause more errors.
- **LRU**: Least Recently Used, the one that was called the longest ago. It can be done with counters or with a pulse of page numbers (better).
	- LRU-Approximation, a reference bit (1) that is copied every n clock ticks into a shift register
	- NRU: Not Recently Used, choose according to the reference bit (highest priority) and dirty.
	- LFU: Least Frequently Used, a counter for each frame that is increased by its access. It can be divided in two to simulate aging.
	- NFU: Not Frequently Used, accumulates the r bit in a counter every n cycles and r=1, choose the one with the smallest counter
- **FINUFO**: First Is Not Used, First Out, clock or second chance, for pages with r=0, a circular queue is performed until r=0 is found.

For the assignment of frames to processes, you can decide to do it equitably (does not take into account the principle of locality or size), proportional to the size of the process (does not take into account the principle of locality) or based on the pages referenced in an interval (difficult to implement).

The latter would involve saving the number of recently referenced pages in a set (working set).

The Page Fault Frequency (PFF) algorithm is based on the frequency of the process of skipping pagefaults, which is based on approximation and an upper and lower bound for the number of frames stored.

In the replacement it can be done locally, the candidates are frames of the WS process or globally, all the frames are candidates.

In Linux the replacement algorithm is Clock+LFU.

## 5.3 Trashing
When the sum of the pages used by the processors in the system is greater than the number of frames. If the point is reached where the “battle” to obtain a referenced page is constant, CPU usage drops significantly and is considered a state of thrashing. Reducing the degree of programs with their suspension can cure this problem.