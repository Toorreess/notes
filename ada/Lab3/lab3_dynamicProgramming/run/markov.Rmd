---
title: "Maximum Likelihood Estimation in Hidden Markov Models"
subtitle: "The Viterbi Algorithm"
author: "Jose Torres Postigo"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    fig_caption: yes
    toc: false
    extra_dependencies: ["placeins"]
papersize: a4
fontsize: 11pt
geometry:
  - top=1in
  - bottom=1in
  - left=1in
  - right=1in
params: 
  algorithm: "Viterbi"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

# Objectives

The goal of this lab excercise is to practise with dynamic programming, a technique where an algorithmic problem is broken down into sub-problems, the results are saved an then the sub-problems are optimized to find the optimal solution. 

For this project, it will be implemented the Viterbi's algorithm for maximum likelihood estimation of the internal states of a hidden Markov Model. This algorithm is a dynamic programming algorithm for decoding convolutional codes, proposed by A.J. Viterbi.

A Hidden Markov Model $HMM(S,\ O,\ \pi,\ \rho,\ \omega)$ is a probabilistic model of a system with hidden variables, based on the Markov Model, where:
- $S = \{s_1,\ ...,\ s_n \}$ are the **states** (values of the random variable).

- $\pi = \{\pi_1,\ ...,\ \pi_n\}$ are the **marginal probabilities** of each state.

- $\rho = \{\rho\ _{ij}\ |\ 1\le i,j \le n\}$ are the **transition probabilities** among the states.

- $O = \{o_1,\ ...,\ o_n \}$ are the possible observable **outputs** of the system.

- $\omega = \{\omega\ _{ij}\ |\ 1 \le i \le n,1 \le j \le m \}$ are the **output probabilities**, i.e. $\omega\ _{ij}$ is the  probability of observing output $o_j$ if the system is in state $s_i$.

After the implementation, it will be a subject matter the results obtained: number of states, number of observations and time taken for each execution.

# Experimental Setup

For this experiment, the Viterbi's algorithm will be run through a class given by the lecturer, which is going to run tests of it using hidden Markov models of increasing number of states and sequences of observations of increasing length. The initial values for these tests are 8 states and 256 observations, up to 202 states and 6561 observations.

\begin{table}[!h]
\caption{Computational environment considered.}
\begin{tabular}{lp{0.8\linewidth}}
\hline
CPU       & Intel® Core™ i5 11600KF, 16GB \\
OS        & Windows 11 Home 22H2 \\
Java      & openjdk 17.0.7 2023-04-18 \\
\hline
\end{tabular}
\label{tab:conf}
\end{table}


# Empirical Results

A summary of the experimental results is provided in Table \@ref(tab:data-summary) in the Appendix.

It can be seen in this graph that the execution time increases with the number of states. Because the Viterbi's algorithm needs to consider all possible sequences of states in order to find the most likely sequence, the more states are there, the more possible sequences exists and the longer the algorithm takes to compute the result.

The graph shows that the execution time increases also with the number of possible observable outputs. This is because the Viterbi's algorithm needs to consider all possible sequences of states for the entire observation sequence. The greater the number of observations, the greater is the number of possible sequences, and it will take longer to the algorithm to find the most likely one.

Also, The graph suggests that the Viterbi algorithm can be used efficiently for sequences of observations with a moderate number of states, such as 05 or 100 states. However, the algorithm may become computationally expensive for a large number of observations and a large number of states, such as 6\,000 observations with 200 states, which takes almost a second to compute.

```{r load-data}
read_stats <- function  (name) {
  # Reading the data
  filename <- paste(name, '-stats.txt', sep="")
  rawdata <- read.table(filename)
  # Get range values
  num_states <- unique(rawdata$V1)
  observation_lengths <- unique(rawdata$V2)
  # Get averages
  stderr <- function(x) sd(x)/sqrt(length(x))
  dataframe <- data.frame(
    "states"       = rawdata[,1],
    "observations" = rawdata[,2],
    "tavg"         = apply(t(rawdata)[c(-1,-2),], 2, median)
  ) 
  observation_length_values <- length(observation_lengths)
  num_states_values <- length(num_states)
  n <- dim(dataframe)[1];
  c0 <- log(dataframe$tavg[n]/dataframe$tavg[n-observation_length_values])/log(dataframe$states[n]/dataframe$states[n-observation_length_values])
  b0 <- log(dataframe$tavg[n]/dataframe$tavg[n-1])/log(dataframe$observations[n]/dataframe$observations[n-1])
  a0 <- dataframe$tavg[n]/(dataframe$observations[n]^b0 * dataframe$states[n]^c0)
  datafit = nls(tavg ~ a * observations^b * states^c , data=dataframe, start = list(a = a0, b = b0, c = c0))
  dataframe$predicted <- predict(datafit, dataframe)

  datalist <- list(data = dataframe, datafit = datafit)
}

experimentaldata <- read_stats(params$algorithm)

```


```{r show-time, fig.cap="Time as a function of the number of states for different number of observations. The dotted lines show a fit to a power model $t = a\\cdot {\\rm observations}^b \\cdot {\\rm states}^c$."}
library(ggplot2)
library(scales)

graphical_representation <- function (dataframe, name) {
  fig <- ggplot(data = dataframe, aes(x = states, y = tavg, colour = observations, group = observations)) + 
    theme_bw()  +
    xlab ("#states") +
    ggtitle (paste(name, "algorithm for different sequences of observations")) +
    geom_point(shape=21, size=3,  fill="white") +
    geom_line(aes(x=states, y = predicted), linetype="dotted") +
    scale_y_continuous(name = "time (s)", labels = scales::number_format(accuracy = 0.01)) +
    theme(legend.justification = c(0, 1), 
          legend.position = c(0, .99), 
          legend.box.margin=margin(c(5,5,5,5)))
    
  
  show (fig)
  }

graphical_representation(experimentaldata$data, params$algorithm)
```

# Discussion

The empirical results I obtained highly match the theoretical predictions in general as the standard error is very low. There are some measurements which are not as fitted to the model as it should be, but this does not necessarily mean that there is a problem with the model nor with the implementation, but with the Hardware used.

The Viterbi algorithm seems to be a a powerful tool for finding the most likely sequence of hidden states for a wide range of sequences of observations. Also it seems to be computationally expensive with a large number of states and observations, so it should be taken into consideration using a more efficient algorithm or reducing the number of states and/or observations in these cases.


\FloatBarrier

\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}

# Appendix

## Data Summary 

```{r data-summary}
library(knitr)
kable(experimentaldata$data[c("states", "observations", "tavg")], "pipe",
      col.names = c("#states", "#observations", "time (s)"), 
      caption = "Summary of the experimental results") 
```

## Model Fitting

```{r}
show(summary(experimentaldata$datafit))
```

