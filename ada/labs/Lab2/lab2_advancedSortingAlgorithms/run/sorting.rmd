---
title: "Divide-and-Conquer Sorting Algorithms"
author: "Jose Torres Postigo"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    fig_caption: yes
    toc: false
    extra_dependencies: ["placeins"]
papersize: a4
fontsize: 11pt
geometry:
  - top=1in
  - bottom=1in
  - left=1.5in
  - right=1.5in
params:
  algorithm: ["selection", "mergesort", "quicksort"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Objectives
The goal of this lab exercise is to practice with the Divide and Conquer method for implementing algorithms, considering the problem of sorting a list of elements. For this lab session, it is requested to implement three sorting methods: QuickSort, MergeSort and SelectionSort, this last one for comparison purposes.

There will be also an analysis for every algorithm implementing regarding the time required for sorting lists, depending on the number of elements contained in the list.

# Experimental Setup

For this experiment, every algorithm is run with through a class given by the lecturer, which is going to feed the algorithm with lists and time how long the algorithm takes to accomplish the sort of the list.

For the QuickSort and Mergesort, the initial size of the list given is 100\,000, increasing over every iteration up to 2\,262\,718. For the SelectionSort, this initial value is 2\,000 up to 45\,233.

\begin{table}[!h]
\caption{Computational environment considered.}
\begin{tabular}{lp{0.8\linewidth}}
\hline
CPU       & Intel® Core™ i5 11600KF, 16GB\\
OS        & Windows 11 Home 22H2\\
Java      & openjdk 17.0.7 2023-04-18\\
\hline
\end{tabular}
\label{tab:conf}
\end{table}


# Empirical Results

A summary of the experimental results is provided in Tables \@ref(tab:mergesort)--\@ref(tab:quicksort) in the Appendix, along with the statistical fitting of the data to different growth models. 

In Figure \@ref(fig:selection) can be seen that the rate of growth for SelectionSort algorithm is much higher than the following algorithms. It takes the same time to sort a list of 40\,000 elements with SelectionSort as a list of 750\,000 elements aprox. with MergeSort, or a list of 1\,125\,000 elements with QuickSort. Thus, is not interesting to take a look at this algorithm for this lab session, only for comparative purposes.

In Figure \@ref(fig:mergesort) we can see a exponential growth with a lower rate than the previous algorithm. With a complexity of $\Theta(n \log n)$, the algorithm can sort lists with 2\,000\,000 elements in less than 4 seconds. Comparing the line graph with the empirical measurement obtained, can be said that is very ccurate for almost every result obtained.

Taking a look at Figure \@ref(fig:quicksort), it can be said that this is the quickest sorting algorithm by far. This algorithm takes about 2 seconds for sorting a 2\,000\,000 elements list. This is half of the time it takes the MergeSort algorithm, even though they have the same time complexity $\Theta(n \log n)$

Taking a look at the summary tables of the experimental results for the algorithms, we can certainly say that is very precise the model fittings as the standard errors are almost insignificant.

```{r}
read_stats <- function  (filename) {
  # Reading the data
  rawdata = read.table(filename)
  # Compute basic statistics 
  stderr <- function(x) sd(x)/sqrt(length(x))
  m <- dim(rawdata)[1];
  n <- dim(rawdata)[2];
  dataframe <- data.frame(
    "size" = rawdata[,1],
    "val" = apply(t(rawdata)[-1,], 2, mean),
    "valerr" = apply(t(rawdata)[-1,], 2, stderr)
  )
  
  # power-law fit
  b0 <- log(dataframe$val[m]/dataframe$val[m-1])/log(dataframe$size[m]/dataframe$size[m-1])
  a0 <- dataframe$val[m]/(dataframe$size[m]^b0)
  datafit1 <- nls(val ~ a * size ^ b, data=dataframe, start = list(a = a0, b = b0))
  fitfun1 <- function(x) predict(datafit1, newdata=data.frame(size=x))
  # power-law fit
  b0 <- log((dataframe$val[m]/log(dataframe$size[m]))/(dataframe$val[m-1]/log(dataframe$size[m-1])))/log(dataframe$size[m]/dataframe$size[m-1])
  a0 <- dataframe$val[m]/(dataframe$size[m]^b0*log(dataframe$size[m]))
  datafit2 <- nls(val ~ a * size ^ b * log(size), data=dataframe, start = list(a = a0, b = b0))
  fitfun2 <- function(x) predict(datafit2, newdata=data.frame(size=x))  
  xx <- seq(1, max(dataframe$size)*2, by = max(dataframe$size)/1000)
  data_projection <- data.frame(
    "size" = xx,
    "val1" = fitfun1(xx),
    "val2" = fitfun2(xx)
  )
  datalist <-list(data = dataframe, fit1 = datafit1, f1 = fitfun1, fit2 = datafit2, f2 = fitfun2, predict = data_projection)
}


algorithm_data <- list()
for (alg in params$algorithm) {
  data <- read_stats(paste(alg, ".txt", sep=""))
  algorithm_data[[alg]] <- data
  
}
```


```{r tc, fig.cap=paste(sprintf("\\label{fig:%s} ", params$algorithm), "Time required for sorting lists of increasing size using ", params$algorithm, ".", sep=""), result="asis", message=FALSE, echo=FALSE, warning=FALSE}
library(ggplot2)

col <- c("data" = "blue", "fit1" = "red", "fit2" = "black")
for (alg in params$algorithm) {
  dataf <- algorithm_data[[alg]]
  figure <- ggplot() + 
    geom_line(data = dataf$predict, aes(x=size, y=val1, colour = "fit1")) +
    geom_line(data = dataf$predict, aes(x=size, y=val2, colour = "fit2")) +
    geom_point(data = dataf$data, shape=21, size=3, aes(x=size, y=val, colour="data"),  fill="white") +
    geom_errorbar(data = dataf$data, aes(x=size, y=val, ymin=val-valerr, ymax=val+valerr, colour="data"), width=1) +
    scale_colour_manual(name = "", values = col, 
                        labels = c("empirical measurement", 
                                   bquote(.(summary(dataf$fit1)$coefficients[1]) ~ n^.(summary(dataf$fit1)$coefficients[2])),
                                   bquote(.(summary(dataf$fit2)$coefficients[1]) ~ n^.(summary(dataf$fit2)$coefficients[2])*log(n))
                        )
    ) + 
    guides(color = guide_legend(override.aes = list(shape = c(1, NA, NA), 
                                                    linetype = c("blank", "solid", "solid"),
                                                    colour = c("blue", "red", "black")
    ) 
    )
    ) +
    coord_cartesian(xlim = c(min(dataf$data$size), max(dataf$data$size)), ylim = c(min(dataf$data$val-dataf$data$valerr),max(dataf$data$val+dataf$data$valerr))) + 
    xlab("number of points") +
    ylab("time (s)") +
    ggtitle(alg) +
    theme_bw() + 
    theme(legend.justification = c(0, 1), 
          legend.position = c(0.05, 1.05), 
          legend.box.margin=margin(c(5,5,5,5)),
          legend.background = element_rect(fill='transparent'))
  #
  cat('\n\n') 
  print(figure)
}
```

\FloatBarrier

# Discussion

Provide your interpretation of the results: discuss whether the results match the theoretical predictions, whether some algorithm is better in practice than others, etc.

The empirical results I obtained highly match the theoretical predictions in general. There are some measurements which are not as fitted to the model as it should be, but this does not necessarily mean that there is a problem with the model nor with the implementation, but it could be the hardware used to take the readings.

Comparing the algorithms discussed in this lab session, there is a clear winner by far: The QuickSort. And not only this is a very fast algorithm, but also surprises me that the implementation is not so hard to work out.

Maybe there is algorithms faster than QuickSort, but only in a setup with the conditions necessary and few elements. On random sets of data, It seems to me that the QuickSort algorithm is the fastest.

\FloatBarrier

\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}

# Appendix

## Numerical Data 

Mean and standard error of sorting lists of different sizes. All times measured in seconds.

```{r data-summary, results='asis'}
library(knitr)
for (alg in params$algorithm) {
  data <- algorithm_data[[alg]]$data
  table_dataframe <- data.frame(
    "size" = data$size,
    "tcmean" = format(data$val, scientific=TRUE, digits = 3),
    "tcstderr" = format(data$valerr, scientific=TRUE, digits = 3)
  )
  
  print(kable(table_dataframe, "pipe",
              col.names = c("list size", "mean time", "std. error"), 
              caption = paste(sprintf("\\label{tab:%s} ", alg), "Summary of the experimental results for ", alg, ".", sep="")))
}


```

## Model Fitting 

```{r}
for (alg in params$algorithm) {
  hyphens <- paste(rep("-", nchar(alg)), collapse="")
  cat(paste(alg, "\n", hyphens, "\n", sep = ""))
  data <- algorithm_data[[alg]]
  show(summary(data$fit1))
  show(summary(data$fit2))
}
```





